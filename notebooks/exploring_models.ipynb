{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808480c",
   "metadata": {},
   "source": [
    "# Comparing Pre-trained, Instruct and Reasoning Models\n",
    "\n",
    "This notebook explores the text generation capabilities of Large Language Models (LLMs) with different training stages, such as the pre-trained and instruct versions of Llama 3 8B and the reasoning model DeepSeek-R1 8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d6900",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004834e8",
   "metadata": {},
   "source": [
    "### Llama 3 8B Pre-trained\n",
    "\n",
    "Pre-trained models can autocomplete input text, as they are trained on next token prediction from vast volumes of raw unlabeled text.\n",
    "\n",
    "However, they often struggle to follow human instructions and to handle complex / multi-step tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a02fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'What is the capital of France? Reverse the answer and delete the vowels' | ollama run llama3:text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ca8e5",
   "metadata": {},
   "source": [
    "### Llama 3 8B Instruct\n",
    "\n",
    "Instruct-tuned models perform better at following human instructions, as they are also trained on prompt-response datasets.\n",
    "\n",
    "However, they sometimes struggle with complex / multi-step tasks (especially smaller models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'What is the capital of France? Reverse the answer and delete the vowels' | ollama run llama3:instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b146d",
   "metadata": {},
   "source": [
    "### DeepSeek-R1 8B\n",
    "\n",
    "Reasoning models perform better on complex tasks, as they are also trained on Chain-of-Thought prompts and reward signals.\n",
    "\n",
    "However, their detailed inner monologue can be excessively long and redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d933af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'What is the capital of France? Reverse the answer and delete the vowels' | ollama run deepseek-r1:8b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
