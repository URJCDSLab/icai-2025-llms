{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808480c",
   "metadata": {},
   "source": [
    "# Exploring Prompt Engineering Best Practices\n",
    "\n",
    "This notebook explores some fundamental guidelines and best practices for effective prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004834e8",
   "metadata": {},
   "source": [
    "## Prompt Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c871031",
   "metadata": {},
   "source": [
    "### Prompt 1: Basic Zero-Shot \n",
    "\n",
    "The first prompts users tend to give to models are very simple and straightforward, with no additional context, constraints, or examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa28ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Classify the sentiment of \"The delivery was extremely late, but the product was excellent\".'\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"{prompt}\" | ollama run llama3:instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b371d6",
   "metadata": {},
   "source": [
    "### Prompt 2: Adding Constraints + Delimeters \n",
    "\n",
    "However, by delimiting the input and restricting the range of possible answers, we can reduce ambiguity and produce more structured and predictable responses for automatized systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the text below. Answer only with the word \"Positive\", \"Negative\", or \"Neutral\".\n",
    "\n",
    "Text: \"The delivery was late, but the product was excellent\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444118ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"{prompt}\" | ollama run llama3:instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bad0d4",
   "metadata": {},
   "source": [
    "### Prompt 3A: Adding More Context\n",
    "\n",
    "Providing more context about the meaning of the labels can improve model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3035163",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"# Persona\n",
    "You are a customer satisfaction analyst. Your task is to evaluate customer reviews and determine overall sentiment toward services and products.\n",
    "\n",
    "# Instructions\n",
    "Classify each review as \"Positive\", \"Negative\", or \"Neutral\" based on the customer's tone and experience.\n",
    "- Positive: The review clearly expresses satisfaction.\n",
    "- Negative: The review clearly expresses dissatisfaction.\n",
    "- Neutral: The review expresses mixed feelings or maintains an indifferent tone.\n",
    "Answer only with the word \"Positive\", \"Negative\", or \"Neutral\".\n",
    "\n",
    "Review: \"The delivery was extremely late, but the product was excellent\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"{prompt}\" | ollama run llama3:instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee997592",
   "metadata": {},
   "source": [
    "### Prompt 3B: Adding Examples (Few-Shot)\n",
    "\n",
    "Also, providing input examples and their corresponding labels can improve end predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the text below. Answer only with the word \"Positive\", \"Negative\", or \"Neutral\".\n",
    "\n",
    "Text: \"The phone case fits perfectly and feels super durable.\"\n",
    "Answer: Positive\n",
    "\n",
    "Text: \"It feels cheap, but it does its job perfectly.\"\n",
    "Answer: Neutral\n",
    "\n",
    "Text: \"It is a terrible product. It broke after only two days!\"\n",
    "Answer: Negative\n",
    "\n",
    "Text: \"The delivery was extremely late, but the product was excellent\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"{prompt}\" | ollama run llama3:instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009d2c0",
   "metadata": {},
   "source": [
    "### Prompt 3AB: Adding More Context and Examples \n",
    "\n",
    "**TODO**: Try combining the previous two prompts, which include a clearer explanation of the label\n",
    "meanings, as well as examples of inputs and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
