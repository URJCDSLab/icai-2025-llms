{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808480c",
   "metadata": {},
   "source": [
    "# Exploring How a Decoder Works\n",
    "\n",
    "This notebook explores the text generation capabilities of early decoder-only Transformer models, such as GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d6900",
   "metadata": {},
   "source": [
    "## Hugging Face Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004834e8",
   "metadata": {},
   "source": [
    "### GPT-2: Hugging Face Pipelines\n",
    "\n",
    "Hugging Face facilitates inference for common NLP tasks with its Pipelines. This includes text generation with popular open-source models, such as GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a02fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = 'An example of a prime number is'\n",
    "generator = pipeline('text-generation', 'gpt2')\n",
    "\n",
    "response = generator(text)\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ca8e5",
   "metadata": {},
   "source": [
    "### GPT-2: Random Responses\n",
    "\n",
    "Decoder-based models usually have a random component in their responses. As a result, the same input can produce multiple different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = generator(text, num_return_sequences=10)\n",
    "responses = [response['generated_text'][0:50] for response in responses]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5f710",
   "metadata": {},
   "source": [
    "### GPT-2: Greedy Generation\n",
    "\n",
    "Apart from setting seeds, another way to make the prediction deterministic is to generate tokens greedily and remove the random component. This method selects the most probable token at each step instead of sampling from a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab233bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator(text, do_sample=False)\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270df2c",
   "metadata": {},
   "source": [
    "### GPT-2: Limit Length \n",
    "\n",
    "Decoder-based models can keep generating tokens (and hallucinating). A very simple way to address this is by limiting the number of tokens generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d114ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator(text, do_sample=False, max_new_tokens=5)\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403b0a5",
   "metadata": {},
   "source": [
    "## Hugging Face AutoTokenizer and AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1585adc",
   "metadata": {},
   "source": [
    "### GPT-2: Pipelines Under the Hood\n",
    "\n",
    "But Hugging Face Pipelines are just an easy way to make predictions. Under the hood, they use a tokenizer and weights from a trained decoder-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ba224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer(\"Hello, how are\", return_tensors='pt')\n",
    "outputs = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f788494",
   "metadata": {},
   "source": [
    "### GPT-2: Tokenization\n",
    "\n",
    "First, raw input text is transformed into a sequence of discrete token IDs, which are then used to look up dense embedding vectors fed into the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d13d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, how are\", return_tensors='pt')\n",
    "print(inputs)\n",
    "\n",
    "idx_to_text = {_id.item(): tokenizer.decode(_id) for _id in inputs['input_ids'][0]}\n",
    "idx_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fba2d",
   "metadata": {},
   "source": [
    "### GPT-2: Predictions\n",
    "\n",
    "The output of a models is another sequence of token IDs, which can be mapped to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=10)[0]\n",
    "idx_to_text = {_id.item(): tokenizer.decode(_id) for _id in outputs}\n",
    "idx_to_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
